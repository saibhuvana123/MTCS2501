{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6Xl-dnKpnS8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "X, y = make_classification(n_samples=100, n_features=10, n_informative=5, n_redundant=2, random_state=42)\n",
        "\n",
        "X_text = [\" \".join([f\"feature{i}_{val}\" for i, val in enumerate(row)]) for row in X]\n",
        "\n",
        "print(\"Sample features (text-like):\")\n",
        "for i in range(5):\n",
        "    print(X_text[i])\n",
        "print(\"\\nSample labels:\")\n",
        "print(y[:5])\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Training data shape:\", X_train.shape)\n",
        "print(\"Testing data shape:\", X_test.shape)\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def compute_cost(X, y, weights, bias):\n",
        "    m = X.shape[0]\n",
        "    h = sigmoid(np.dot(X, weights) + bias)\n",
        "    cost = (-1/m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))\n",
        "    return cost\n",
        "\n",
        "def gradient_descent(X, y, weights, bias, learning_rate, num_iterations):\n",
        "    m = X.shape[0]\n",
        "    costs = []\n",
        "\n",
        "    for _ in range(num_iterations):\n",
        "        h = sigmoid(np.dot(X, weights) + bias)\n",
        "        dw = (1/m) * np.dot(X.T, (h - y))\n",
        "        db = (1/m) * np.sum(h - y)\n",
        "\n",
        "        weights -= learning_rate * dw\n",
        "        bias -= learning_rate * db\n",
        "\n",
        "        cost = compute_cost(X, y, weights, bias)\n",
        "        costs.append(cost)\n",
        "\n",
        "    return weights, bias, costs\n",
        "\n",
        "def predict(X, weights, bias):\n",
        "    h = sigmoid(np.dot(X, weights) + bias)\n",
        "    y_prediction = (h > 0.5).astype(int)\n",
        "    return y_prediction\n",
        "\n",
        "class LogisticRegression:\n",
        "    def __init__(self, learning_rate=0.01, num_iterations=1000):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_iterations = num_iterations\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "        self.costs = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        self.weights, self.bias, self.costs = gradient_descent(X, y, self.weights, self.bias, self.learning_rate, self.num_iterations)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return predict(X, self.weights, self.bias)\n",
        "\n",
        "model = LogisticRegression(learning_rate=0.01, num_iterations=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy on the test set: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBhXwAtm_ZSF",
        "outputId": "39b91782-6a5b-4573-aaa5-14b179d3db2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample features (text-like):\n",
            "feature0_-0.1490366667295299 feature1_3.6397899913839704 feature2_-4.77202522973986 feature3_-0.006653454631237347 feature4_-1.712936631956997 feature5_-2.7458937711934364 feature6_-1.0248226493397838 feature7_4.487370834009874 feature8_-1.1254190077468889 feature9_-1.0148534542694319\n",
            "feature0_2.6435715833985505 feature1_2.2477201422725925 feature2_0.26929568320199326 feature3_-0.20284621967015248 feature4_2.757147072496183 feature5_2.6745462129469226 feature6_-2.0242249704321327 feature7_0.3013079301649735 feature8_0.18053084180543139 feature9_1.4558042487379736\n",
            "feature0_0.3431350510168737 feature1_-0.9457604059068692 feature2_0.5579200883668298 feature3_1.3238748037872132 feature4_-1.249062046897537 feature5_2.2919250286196764 feature6_-0.7443030472470422 feature7_-0.26229478380375704 feature8_1.2056280779528372 feature9_-0.7259418982501984\n",
            "feature0_-0.3003604880886286 feature1_-1.5113535061026575 feature2_-0.6326839257833132 feature3_-0.8047869166603249 feature4_-1.2546416979638844 feature5_2.1845067557704017 feature6_-0.4365713341976043 feature7_-0.9675455093881183 feature8_0.7936579419803536 feature9_-0.21702475634546814\n",
            "feature0_-0.7984390474170536 feature1_3.013816950344355 feature2_0.6640987873968396 feature3_1.3413119197001346 feature4_2.697771390144384 feature5_0.022002639290531256 feature6_0.6335982054283361 feature7_0.6672305710103317 feature8_-1.5240152417270303 feature9_-1.1112188575166537\n",
            "\n",
            "Sample labels:\n",
            "[0 0 1 0 1]\n",
            "Training data shape: (80, 10)\n",
            "Testing data shape: (20, 10)\n",
            "Accuracy on the test set: 0.8000\n"
          ]
        }
      ]
    }
  ]
}